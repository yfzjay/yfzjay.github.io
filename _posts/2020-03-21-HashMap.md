---
layout: post
title: 'HashMap'
date: 2020-03-21
author: yza
color: rgb(255,210,32)
cover: '{{base.siteurl}}/assets/java.jpg'
tags: java java基础
typora-copy-images-to: ..\assets\image
---



##   什么是HashMap？

![img]({{site.baseurl}}/assets/image/640.webp)





![img]({{site.baseurl}}/assets/image/640.webp)





![img]({{site.baseurl}}/assets/image/640.webp)





![img]({{site.baseurl}}/assets/image/640.webp)





————————————





![img]({{site.baseurl}}/assets/image/640.webp)





![img]({{site.baseurl}}/assets/image/640.webp)





![img]({{site.baseurl}}/assets/image/640.webp)





![img]({{site.baseurl}}/assets/image/640-1584760541605.webp)





![img]({{site.baseurl}}/assets/image/640-1584760541615.webp)





![img]({{site.baseurl}}/assets/image/640-1584760541624.webp)





众所周知，HashMap是一个用于存储Key-Value键值对的集合，每一个键值对也叫做**Entry**。这些个键值对（Entry）分散存储在一个数组当中，这个数组就是HashMap的主干。



HashMap数组每一个元素的初始值都是Null。





![img]({{site.baseurl}}/assets/image/640-1584760541667.webp)





对于HashMap，我们最常使用的是两个方法：**Get** 和 **Put**。





#### **1.Put方法的原理**



调用Put方法的时候发生了什么呢？



比如调用 hashMap.put("apple", 0) ，插入一个Key为“apple"的元素。这时候我们需要利用一个哈希函数来确定Entry的插入位置（index）：



**index = Hash（“apple”）**



假定最后计算出的index是2，那么结果如下：





![img]({{site.baseurl}}/assets/image/640.png)





但是，因为HashMap的长度是有限的，当插入的Entry越来越多时，再完美的Hash函数也难免会出现index冲突的情况。比如下面这样：





![img]({{site.baseurl}}/assets/image/640.png)





这时候该怎么办呢？我们可以利用**链表**来解决。



HashMap数组的每一个元素不止是一个Entry对象，也是一个链表的头节点。每一个Entry对象通过Next指针指向它的下一个Entry节点。当新来的Entry映射到冲突的数组位置时，只需要插入到对应的链表即可：



![img]({{site.baseurl}}/assets/image/640.png)





需要注意的是，新来的Entry节点插入链表时，使用的是“头插法”。至于为什么不插入链表尾部，后面会有解释。



#### **2.Get方法的原理**

使用Get方法根据Key来查找Value的时候，发生了什么呢？



首先会把输入的Key做一次Hash映射，得到对应的index：



**index = Hash（“apple”）**



由于刚才所说的Hash冲突，同一个位置有可能匹配到多个Entry，这时候就需要顺着对应链表的头节点，一个一个向下来查找。假设我们要查找的Key是“apple”：



![img]({{site.baseurl}}/assets/image/640-1584760541675.webp)





第一步，我们查看的是头节点Entry6，Entry6的Key是banana，显然不是我们要找的结果。



第二步，我们查看的是Next节点Entry1，Entry1的Key是apple，正是我们要找的结果。



之所以把Entry6放在头节点，是因为HashMap的发明者认为，**后插入的Entry被查找的可能性更大**。

![img]({{site.baseurl}}/assets/image/640-1584760600331.webp)





![img]({{site.baseurl}}/assets/image/640-1584760600332.webp)





![img]({{site.baseurl}}/assets/image/640-1584760600333.webp)


![img]({{site.baseurl}}/assets/image/640-1584760631637.webp)





![img]({{site.baseurl}}/assets/image/640-1584760631644.webp)





![img]({{site.baseurl}}/assets/image/640-1584760652894.webp)

![img]({{site.baseurl}}/assets/image/640-1584760783868.webp)





![img]({{site.baseurl}}/assets/image/640-1584760783873.webp)





![img]({{site.baseurl}}/assets/image/640-1584760783883.webp)





![img]({{site.baseurl}}/assets/image/640-1584760783895.webp)





![img]({{site.baseurl}}/assets/image/640-1584760783898.webp)





之前说过，从Key映射到HashMap数组的对应位置，会用到一个Hash函数：



**index = Hash（“apple”）**



如何实现一个尽量均匀分布的Hash函数呢？我们通过利用Key的HashCode值来做某种运算。



![img]({{site.baseurl}}/assets/image/640-1584760783912.webp)





**index = HashCode（Key） % Length ?**





![img]({{site.baseurl}}/assets/image/640-1584760783924.webp)





如何进行位运算呢？有如下的公式（Length是HashMap的长度）：



**index = HashCode（Key） & （Length- 1）** 



下面我们以值为“book”的Key来演示整个过程：



1.计算book的hashcode，结果为十进制的3029737，二进制的101110001110101110 1001。



2.假定HashMap长度是默认的16，计算Length-1的结果为十进制的15，二进制的1111。



3.把以上两个结果做**与运算**，101110001110101110 1001 & 1111 = 1001，十进制是9，所以 index=9。



可以说，Hash算法最终得到的index结果，完全取决于Key的Hashcode值的最后几位。







![img]({{site.baseurl}}/assets/image/640-1584760783929.webp)





![img]({{site.baseurl}}/assets/image/640-1584760783939.webp)





假设HashMap的长度是10，重复刚才的运算步骤：





![img]({{site.baseurl}}/assets/image/640-1584760783947.webp)





单独看这个结果，表面上并没有问题。我们再来尝试一个新的HashCode 101110001110101110 **1011** ：





![img]({{site.baseurl}}/assets/image/640-1584760783972.png)





让我们再换一个HashCode 101110001110101110 **1111** 试试 ：





![img]({{site.baseurl}}/assets/image/640-1584760783960.webp)



是的，虽然HashCode的倒数第二第三位从0变成了1，但是运算的结果都是1001。也就是说，当HashMap长度为10的时候，有些index结果的出现几率会更大，而有些index结果永远不会出现（比如0111）！



这样，显然不符合Hash算法均匀分布的原则。



反观长度16或者其他2的幂，Length-1的值是所有二进制位全为1，这种情况下，index的结果等同于HashCode后几位的值。只要输入的HashCode本身分布均匀，Hash算法的结果就是均匀的。

## HashMap的高并发问题

![img]({{site.baseurl}}/assets/image/640-1584760783966.webp)



![漫画：高并发下的HashMap]({{site.baseurl}}/assets/image/1572424484-bba70c0a01465c9.jpeg)







![漫画：高并发下的HashMap]({{site.baseurl}}/assets/image/1572424484-0dada4a2185396f.jpeg)





![漫画：高并发下的HashMap]({{site.baseurl}}/assets/image/1572424484-13de2ee20a63bd4.jpeg)





![漫画：高并发下的HashMap]({{site.baseurl}}/assets/image/1572424484-13de2ee20a63bd4-1.jpeg)





![漫画：高并发下的HashMap]({{site.baseurl}}/assets/image/1572424485-c5b270a18dbd168.jpeg)





![漫画：高并发下的HashMap]({{site.baseurl}}/assets/image/1572424485-a24a7fcf512a6a1.jpeg)





HashMap的容量是有限的。当经过多次元素插入，使得HashMap达到一定饱和度时，Key映射位置发生冲突的几率会逐渐提高。



这时候，HashMap需要扩展它的长度，也就是进行**Resize**。







![漫画：高并发下的HashMap]({{site.baseurl}}/assets/image/1572424485-2a37ad2fb153307.png)





影响发生Resize的因素有两个：

**
**

**1.Capacity**

HashMap的当前长度。上一期曾经说过，HashMap的长度是2的幂。



**2.LoadFactor**

HashMap负载因子，默认值为0.75f。





衡量HashMap是否进行Resize的条件如下：

**
**

**HashMap.Size  >= Capacity * LoadFactor**





![漫画：高并发下的HashMap]({{site.baseurl}}/assets/image/1572424485-390e0b9b6d8167b.jpeg)





![漫画：高并发下的HashMap]({{site.baseurl}}/assets/image/1572424485-5215bc65517a307.jpeg)





**1.扩容**

创建一个新的Entry空数组，长度是原数组的2倍。







**2.ReHash**

遍历原Entry数组，把所有的Entry重新Hash到新数组。为什么要重新Hash呢？因为长度扩大以后，Hash的规则也随之改变。



让我们回顾一下Hash公式：

**index = HashCode（****Key****） & （****Length** **– 1）** 



当原数组长度为8时，Hash运算是和111B做与运算；新数组长度为16，Hash运算是和1111B做与运算。Hash结果显然不同。





**Resize前的HashMap：**

**
**



![漫画：高并发下的HashMap]({{site.baseurl}}/assets/image/1572424486-09c94b54aea1290.png)





**Resize后的HashMap：**

**
**



![漫画：高并发下的HashMap]({{site.baseurl}}/assets/image/1572424486-d0a8ade4d7465f5.png)





**ReHash的Java代码如下：**

**
**

```
/**
 * Transfers all entries from current table to newTable.
 */
void transfer(Entry[] newTable, boolean rehash) {
    int newCapacity = newTable.length;
    for (Entry<K,V> e : table) {
        while(null != e) {
            Entry<K,V> next = e.next;
            if (rehash) {
                e.hash = null == e.key ? 0 : hash(e.key);
            }
            int i = indexFor(e.hash, newCapacity);
            e.next = newTable[i];
            newTable[i] = e;
            e = next;
        }
    }
}
```



![漫画：高并发下的HashMap]({{site.baseurl}}/assets/image/1572424486-645ebfd02dc39e2.jpeg)





![漫画：高并发下的HashMap]({{site.baseurl}}/assets/image/1572424486-a308d1f16e6c764.jpeg)





![漫画：高并发下的HashMap]({{site.baseurl}}/assets/image/1572424487-8d77fd2708d4e23.jpeg)





**注意：下面的内容十分烧脑，请小伙伴们坐稳扶好。**





假设一个HashMap已经到了Resize的临界点。此时有两个线程A和B，在同一时刻对HashMap进行Put操作：







![漫画：高并发下的HashMap]({{site.baseurl}}/assets/image/1572424487-563ed681a60f872.png)





![漫画：高并发下的HashMap]({{site.baseurl}}/assets/image/1572424487-8d77fd2708d4e23.png)





此时达到Resize条件，两个线程各自进行Rezie的第一步，也就是扩容：





![漫画：高并发下的HashMap]({{site.baseurl}}/assets/image/1572424488-9eed90a2714461b.png)





这时候，两个线程都走到了ReHash的步骤。让我们回顾一下ReHash的代码：



![漫画：高并发下的HashMap]({{site.baseurl}}/assets/image/1572424488-88345f29a46aefd.png)



假如此时线程B遍历到Entry3对象，刚执行完红框里的这行代码，线程就被挂起。对于线程B来说：



**e = Entry3**

**next = Entry2**

**
**

这时候线程A畅通无阻地进行着Rehash，当ReHash完成后，结果如下（图中的e和next，代表线程B的两个引用）：





![漫画：高并发下的HashMap]({{site.baseurl}}/assets/image/1572424488-88345f29a46aefd-1.png)





直到这一步，看起来没什么毛病。接下来线程B恢复，继续执行属于它自己的ReHash。线程B刚才的状态是：





**e = Entry3**

**next = Entry2**

**
**



![漫画：高并发下的HashMap]({{site.baseurl}}/assets/image/1572424489-0ce457966482ef8.png)

当执行到上面这一行时，显然 i = 3，因为刚才线程A对于Entry3的hash结果也是3。





![漫画：高并发下的HashMap]({{site.baseurl}}/assets/image/1572424489-a038ecc86314559.png)



我们继续执行到这两行，Entry3放入了线程B的数组下标为3的位置，并且**e指向了Entry2**。此时e和next的指向如下：



**e = Entry2**

**next = Entry2**



整体情况如图所示：







![漫画：高并发下的HashMap]({{site.baseurl}}/assets/image/1572424489-7b0c5690d364cde.png)





接着是新一轮循环，又执行到红框内的代码行：



![漫画：高并发下的HashMap]({{site.baseurl}}/assets/image/1572424490-70860a5b905594b.png)



**e = Entry2**

**next = Entry3**



整体情况如图所示：





![漫画：高并发下的HashMap]({{site.baseurl}}/assets/image/1572424490-70860a5b905594b-1.png)







接下来执行下面的三行，用头插法把Entry2插入到了线程B的数组的头结点：





![漫画：高并发下的HashMap]({{site.baseurl}}/assets/image/1572424490-a5533d3036d1555.png)





整体情况如图所示：



![漫画：高并发下的HashMap]({{site.baseurl}}/assets/image/1572424491-c0c6cb15f20ccf4.png)





第三次循环开始，又执行到红框的代码：



![漫画：高并发下的HashMap]({{site.baseurl}}/assets/image/1572424491-139e79dbfe86dca.png)

**
**

**e = Entry3**

**next = Entry3.next = null**

**
**

最后一步，当我们执行下面这一行的时候，见证奇迹的时刻来临了：





![漫画：高并发下的HashMap]({{site.baseurl}}/assets/image/1572424491-69db984b925cfba.png)



**newTable[i] = \**Entry2\****

**e = Entry3**

***\**\*Entry2.next = Entry3\*\**\***

**Entry3.next = Entry2**

**
**

**链表出现了环形！**

**
**

整体情况如图所示：





![漫画：高并发下的HashMap]({{site.baseurl}}/assets/image/1572424491-d40fa642005fe3a.png)





此时，问题还没有直接产生。当调用Get查找一个不存在的Key，而这个Key的Hash结果恰好等于3的时候，由于位置3带有环形链表，所以程序将会进入**死循环**！



这种情况，不禁让人联想到一道经典的面试题：

[漫画算法：如何判断链表有环？]({{site.baseurl}}/2020/03/21/链表有环问题.html)



![漫画：高并发下的HashMap]({{site.baseurl}}/assets/image/1572424492-d9e025a7c750955.jpeg)





![漫画：高并发下的HashMap]({{site.baseurl}}/assets/image/1572424492-45ba377a9129a97.jpeg)







![漫画：高并发下的HashMap]({{site.baseurl}}/assets/image/1572424492-7c8dc8fc80df7f2.jpeg)





![漫画：高并发下的HashMap]({{site.baseurl}}/assets/image/1572424493-9bf9d6cf122efcd.jpeg)





![漫画：高并发下的HashMap]({{site.baseurl}}/assets/image/1572424493-1724628feda87da.jpeg)





**1.Hashmap在插入元素过多的时候需要进行Resize，Resize的条件是**

**HashMap.Size  >= Capacity * LoadFactor。**

***

**2.Hashmap的Resize包含扩容和ReHash两个步骤，ReHash在并发的情况下可能会形成链表环。**





# 什么是ConcurrentHashMap？

![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424466-e48062254055ca0.jpeg)





![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424466-779afb0ddfcfb68.jpeg)





![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424466-1c866b66f603f10.jpeg)





![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424466-7d492c3bb3df5fb.jpeg)





————————————





![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424466-779afb0ddfcfb68-1.jpeg)





![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424467-8923ab47d1da889.jpeg)





![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424467-75a17dd83144fa4.jpeg)





![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424470-a4eba9b184c19ca.jpeg)





![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424470-a05096e779d49da.jpeg)





![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424471-eac56b0b9bd0caf.jpeg)





![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424471-61f1ad03ff260b4.jpeg)





![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424471-c7c33f4fd5e0295.jpeg)





![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424471-eac56b0b9bd0caf-1.jpeg)





![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424472-faab9315c020947.jpeg)





![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424472-ea0cb3ba59f731d.jpeg)





![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424472-91b43f61f85b8c9.jpeg)





————————————





![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424472-ea0cb3ba59f731d-1.jpeg)





![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424472-44a81bc0f8e16fa.jpeg)



我们来简单回顾一下HashMap的结构：



![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424473-86f83fcd8272762.png)





简单来说，HashMap是一个Entry对象的数组。数组中的每一个Entry元素，又是一个链表的头节点。



Hashmap不是线程安全的。在高并发环境下做插入操作，有可能出现下面的环形链表：





![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424473-fd41f2e81b48c12.png)





![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424473-4bb4361d1414d9d.jpeg)







![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424473-fd41f2e81b48c12.jpeg)





![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424474-203977a7791d16a.png)





![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424474-f459592aecf065a.png)







![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424475-8bd475f2f103870.jpeg)







![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424475-ffc0298cc5ca0c9.jpeg)





![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424475-6517ff63ee3b5d6.jpeg)





Segment是什么呢？Segment本身就相当于一个HashMap对象。



同HashMap一样，Segment包含一个HashEntry数组，数组中的每一个HashEntry既是一个键值对，也是一个链表的头节点。



单一的Segment结构如下：





![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424475-73637d136737052.png)





像这样的Segment对象，在ConcurrentHashMap集合中有多少个呢？有2的N次方个，共同保存在一个名为segments的数组当中。



因此整个ConcurrentHashMap的结构如下：







![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424476-11d08d6d49e42a9.png)





可以说，ConcurrentHashMap是一个二级哈希表。在一个总的哈希表下面，有若干个子哈希表。



这样的二级结构，和数据库的水平拆分有些相似。







![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424476-11d08d6d49e42a9.jpeg)





![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424476-f3689baa3174852.jpeg)





![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424477-5bca9a8381ec389.jpeg)





**Case1：不同Segment的并发写入**

**
**

**
**



![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424477-4ac52aab6b164e6.png)





不同Segment的写入是可以并发执行的。





**Case2：同一Segment的一写一读**

**
**

**
**



![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424477-027e752b4dbab82.png)





同一Segment的写和读是可以并发执行的。





**Case3：同一Segment的并发写入**

**
**

**
**



![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424477-4ac52aab6b164e6-1.png)





Segment的写入是需要上锁的，因此对同一Segment的并发写入会被阻塞。





由此可见，ConcurrentHashMap当中每个Segment各自持有一把锁。在保证线程安全的同时降低了锁的粒度，让并发操作效率更高。







![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424478-1f0e694ae57de82.jpeg)





![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424478-89ea882de73a6f0.jpeg)





**Get方法：**

**
**

1.为输入的Key做Hash运算，得到hash值。



2.通过hash值，定位到对应的Segment对象



3.再次通过hash值，定位到Segment当中数组的具体位置。





**Put方法：**

**
**

1.为输入的Key做Hash运算，得到hash值。



2.通过hash值，定位到对应的Segment对象



3.获取可重入锁



4.再次通过hash值，定位到Segment当中数组的具体位置。



5.插入或覆盖HashEntry对象。



6.释放锁。





![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424478-ebedd3e1d29ebe1.jpeg)





![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424479-ac8f921f96b3846.jpeg)





Size方法的目的是统计ConcurrentHashMap的总元素数量， 自然需要把各个Segment内部的元素数量汇总起来。



但是，如果在统计Segment元素数量的过程中，已统计过的Segment瞬间插入新的元素，这时候该怎么办呢？





![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424479-8463b88b89d2ceb.png)





![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424479-b7e82984464ffc8.png)





![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424479-52c3d5e7b8be4e6.png)





![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424479-f4bc1826a5dd606.jpeg)





ConcurrentHashMap的Size方法是一个嵌套循环，大体逻辑如下：



1.遍历所有的Segment。



2.把Segment的元素数量累加起来。



3.把Segment的修改次数累加起来。



4.判断所有Segment的总修改次数是否大于上一次的总修改次数。如果大于，说明统计过程中有修改，重新统计，尝试次数+1；如果不是。说明没有修改，统计结束。



5.如果尝试次数超过阈值，则对每一个Segment加锁，再重新统计。



6.再次判断所有Segment的总修改次数是否大于上一次的总修改次数。由于已经加锁，次数一定和上次相等。



7.释放锁，统计结束。





官方源代码如下：



```
public int size() {
    // Try a few times to get accurate count. On failure due to
   // continuous async changes in table, resort to locking.
   final Segment<K,V>[] segments = this.segments;
    int size;
    boolean overflow; // true if size overflows 32 bits
    long sum;         // sum of modCounts
    long last = 0L;   // previous sum
    int retries = -1; // first iteration isn't retry
    try {
        for (;;) {
            if (retries++ == RETRIES_BEFORE_LOCK) {
                for (int j = 0; j < segments.length; ++j)
                    ensureSegment(j).lock(); // force creation
            }
            sum = 0L;
            size = 0;
            overflow = false;
            for (int j = 0; j < segments.length; ++j) {
                Segment<K,V> seg = segmentAt(segments, j);
                if (seg != null) {
                    sum += seg.modCount;
                    int c = seg.count;
                    if (c < 0 || (size += c) < 0)
                        overflow = true;
                }
            }
            if (sum == last)
                break;
            last = sum;
        }
    } finally {
        if (retries > RETRIES_BEFORE_LOCK) {
            for (int j = 0; j < segments.length; ++j)
                segmentAt(segments, j).unlock();
        }
    }
    return overflow ? Integer.MAX_VALUE : size;
}
```



为什么这样设计呢？这种思想和乐观锁悲观锁的思想如出一辙。



为了尽量不锁住所有Segment，首先乐观地假设Size过程中不会有修改。当尝试一定次数，才无奈转为悲观锁，锁住所有Segment保证强一致性。







![漫画：什么是ConcurrentHashMap？]({{base.siteurl}}/assets/image/1572424480-06f66e3f23069a6.jpeg)





**几点说明：**



\1. 这里介绍的ConcurrentHashMap原理和代码，都是基于Java1.7的。在Java8中会有些许差别。



2.ConcurrentHashMap在对Key求Hash值的时候，为了实现Segment均匀分布，进行了两次Hash。有兴趣的朋友可以研究一下源代码。